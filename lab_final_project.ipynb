{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Q3MIABPajQ8e",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2b37f5-fa34-48fd-fbc2-46ed21a47899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libdivsufsort3\n",
            "Suggested packages:\n",
            "  hmmer-doc\n",
            "The following NEW packages will be installed:\n",
            "  hmmer libdivsufsort3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 1,198 kB of archives.\n",
            "After this operation, 7,621 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libdivsufsort3 amd64 2.0.1-5 [42.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 hmmer amd64 3.3.2+dfsg-1 [1,155 kB]\n",
            "Fetched 1,198 kB in 1s (1,358 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libdivsufsort3:amd64.\n",
            "(Reading database ... 121918 files and directories currently installed.)\n",
            "Preparing to unpack .../libdivsufsort3_2.0.1-5_amd64.deb ...\n",
            "Unpacking libdivsufsort3:amd64 (2.0.1-5) ...\n",
            "Selecting previously unselected package hmmer.\n",
            "Preparing to unpack .../hmmer_3.3.2+dfsg-1_amd64.deb ...\n",
            "Unpacking hmmer (3.3.2+dfsg-1) ...\n",
            "Setting up libdivsufsort3:amd64 (2.0.1-5) ...\n",
            "Setting up hmmer (3.3.2+dfsg-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  aspell aspell-en bubblewrap dictionaries-common enchant-2 evince\n",
            "  evince-common fonts-droid-fallback fonts-noto-mono fonts-urw-base35\n",
            "  gnome-desktop3-data gsettings-desktop-schemas gstreamer1.0-plugins-base\n",
            "  hunspell-en-us libaspell15 libcdparanoia0 libdjvulibre-text libdjvulibre21\n",
            "  libenchant-2-2 libevdocument3-4 libevview3-3 libgnome-desktop-3-19 libgs9\n",
            "  libgs9-common libgspell-1-2 libgspell-1-common libgxps2 libhandy-1-0\n",
            "  libhunspell-1.7-0 libidn12 libijs-0.35 libjbig2dec0 libkpathsea6\n",
            "  libnautilus-extension1a libpoppler-glib8 libsecret-1-0 libsecret-common\n",
            "  libspectre1 libsynctex2 libtext-iconv-perl libvisual-0.4-0 libxkbregistry0\n",
            "  poppler-data session-migration\n",
            "Suggested packages:\n",
            "  aspell-doc spellutils wordlist gvfs nautilus-sendto fonts-noto\n",
            "  fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre hunspell\n",
            "  openoffice.org-hunspell | openoffice.org-core libenchant-2-voikko\n",
            "  libvisual-0.4-plugins poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  aspell aspell-en bubblewrap dictionaries-common enchant-2 evince\n",
            "  evince-common fonts-droid-fallback fonts-noto-mono fonts-urw-base35\n",
            "  gnome-desktop3-data gsettings-desktop-schemas gstreamer1.0-plugins-base\n",
            "  hmmer-doc hunspell-en-us libaspell15 libcdparanoia0 libdjvulibre-text\n",
            "  libdjvulibre21 libenchant-2-2 libevdocument3-4 libevview3-3\n",
            "  libgnome-desktop-3-19 libgs9 libgs9-common libgspell-1-2 libgspell-1-common\n",
            "  libgxps2 libhandy-1-0 libhunspell-1.7-0 libidn12 libijs-0.35 libjbig2dec0\n",
            "  libkpathsea6 libnautilus-extension1a libpoppler-glib8 libsecret-1-0\n",
            "  libsecret-common libspectre1 libsynctex2 libtext-iconv-perl libvisual-0.4-0\n",
            "  libxkbregistry0 poppler-data session-migration\n",
            "0 upgraded, 45 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 22.2 MB of archives.\n",
            "After this operation, 82.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libtext-iconv-perl amd64 1.7-7build3 [14.3 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libaspell15 amd64 0.60.8-4build1 [325 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 dictionaries-common all 1.28.14 [185 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell amd64 0.60.8-4build1 [87.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 aspell-en all 2018.04.16-0-1 [299 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 bubblewrap amd64 0.6.1-1 [46.0 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 hunspell-en-us all 1:2020.12.07-2 [280 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhunspell-1.7-0 amd64 1.7.0-4build1 [175 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libenchant-2-2 amd64 2.3.2-1ubuntu2 [50.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 enchant-2 amd64 2.3.2-1ubuntu2 [13.0 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 evince-common all 42.3-0ubuntu3.1 [130 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 session-migration amd64 0.3.6 [9,774 B]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 gsettings-desktop-schemas all 42.0-1ubuntu1 [31.1 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre-text all 3.5.28-2build2 [50.9 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdjvulibre21 amd64 3.5.28-2build2 [624 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgxps2 amd64 0.3.2-2 [60.9 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpoppler-glib8 amd64 22.02.0-2ubuntu0.4 [134 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.6 [751 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.6 [5,031 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 libspectre1 amd64 0.2.10-1 [31.8 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libevdocument3-4 amd64 42.3-0ubuntu3.1 [179 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libcdparanoia0 amd64 3.10.2+debian-14build2 [49.3 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libvisual-0.4-0 amd64 0.4.0-17build2 [108 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gstreamer1.0-plugins-base amd64 1.20.1-1ubuntu0.1 [712 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgspell-1-common all 1.9.1-4 [6,114 B]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgspell-1-2 amd64 1.9.1-4 [56.5 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libevview3-3 amd64 42.3-0ubuntu3.1 [149 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 gnome-desktop3-data all 42.9-0ubuntu1 [23.3 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbregistry0 amd64 1.4.0-1 [14.1 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgnome-desktop-3-19 amd64 42.9-0ubuntu1 [120 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libhandy-1-0 amd64 1.6.1-1 [251 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libnautilus-extension1a amd64 1:42.6-0ubuntu1 [16.2 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsecret-common all 0.20.5-2 [4,278 B]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsecret-1-0 amd64 0.20.5-2 [124 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 evince amd64 42.3-0ubuntu3.1 [301 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 hmmer-doc all 3.3.2+dfsg-1 [769 kB]\n",
            "Fetched 22.2 MB in 1s (15.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 45.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 121972 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../01-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package libtext-iconv-perl.\n",
            "Preparing to unpack .../02-libtext-iconv-perl_1.7-7build3_amd64.deb ...\n",
            "Unpacking libtext-iconv-perl (1.7-7build3) ...\n",
            "Selecting previously unselected package libaspell15:amd64.\n",
            "Preparing to unpack .../03-libaspell15_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Selecting previously unselected package dictionaries-common.\n",
            "Preparing to unpack .../04-dictionaries-common_1.28.14_all.deb ...\n",
            "Adding 'diversion of /usr/share/dict/words to /usr/share/dict/words.pre-dictionaries-common by dictionaries-common'\n",
            "Unpacking dictionaries-common (1.28.14) ...\n",
            "Selecting previously unselected package aspell.\n",
            "Preparing to unpack .../05-aspell_0.60.8-4build1_amd64.deb ...\n",
            "Unpacking aspell (0.60.8-4build1) ...\n",
            "Selecting previously unselected package aspell-en.\n",
            "Preparing to unpack .../06-aspell-en_2018.04.16-0-1_all.deb ...\n",
            "Unpacking aspell-en (2018.04.16-0-1) ...\n",
            "Selecting previously unselected package bubblewrap.\n",
            "Preparing to unpack .../07-bubblewrap_0.6.1-1_amd64.deb ...\n",
            "Unpacking bubblewrap (0.6.1-1) ...\n",
            "Selecting previously unselected package hunspell-en-us.\n",
            "Preparing to unpack .../08-hunspell-en-us_1%3a2020.12.07-2_all.deb ...\n",
            "Unpacking hunspell-en-us (1:2020.12.07-2) ...\n",
            "Selecting previously unselected package libhunspell-1.7-0:amd64.\n",
            "Preparing to unpack .../09-libhunspell-1.7-0_1.7.0-4build1_amd64.deb ...\n",
            "Unpacking libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Selecting previously unselected package libenchant-2-2:amd64.\n",
            "Preparing to unpack .../10-libenchant-2-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package enchant-2.\n",
            "Preparing to unpack .../11-enchant-2_2.3.2-1ubuntu2_amd64.deb ...\n",
            "Unpacking enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Selecting previously unselected package evince-common.\n",
            "Preparing to unpack .../12-evince-common_42.3-0ubuntu3.1_all.deb ...\n",
            "Unpacking evince-common (42.3-0ubuntu3.1) ...\n",
            "Selecting previously unselected package session-migration.\n",
            "Preparing to unpack .../13-session-migration_0.3.6_amd64.deb ...\n",
            "Unpacking session-migration (0.3.6) ...\n",
            "Selecting previously unselected package gsettings-desktop-schemas.\n",
            "Preparing to unpack .../14-gsettings-desktop-schemas_42.0-1ubuntu1_all.deb ...\n",
            "Unpacking gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Selecting previously unselected package libdjvulibre-text.\n",
            "Preparing to unpack .../15-libdjvulibre-text_3.5.28-2build2_all.deb ...\n",
            "Unpacking libdjvulibre-text (3.5.28-2build2) ...\n",
            "Selecting previously unselected package libdjvulibre21:amd64.\n",
            "Preparing to unpack .../16-libdjvulibre21_3.5.28-2build2_amd64.deb ...\n",
            "Unpacking libdjvulibre21:amd64 (3.5.28-2build2) ...\n",
            "Selecting previously unselected package libgxps2:amd64.\n",
            "Preparing to unpack .../17-libgxps2_0.3.2-2_amd64.deb ...\n",
            "Unpacking libgxps2:amd64 (0.3.2-2) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../18-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libpoppler-glib8:amd64.\n",
            "Preparing to unpack .../19-libpoppler-glib8_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking libpoppler-glib8:amd64 (22.02.0-2ubuntu0.4) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../20-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../21-libgs9-common_9.55.0~dfsg1-0ubuntu5.6_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.6) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../22-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../23-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../24-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../25-libgs9_9.55.0~dfsg1-0ubuntu5.6_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.6) ...\n",
            "Selecting previously unselected package libspectre1:amd64.\n",
            "Preparing to unpack .../26-libspectre1_0.2.10-1_amd64.deb ...\n",
            "Unpacking libspectre1:amd64 (0.2.10-1) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../27-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdocument3-4:amd64.\n",
            "Preparing to unpack .../28-libevdocument3-4_42.3-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libevdocument3-4:amd64 (42.3-0ubuntu3.1) ...\n",
            "Selecting previously unselected package libcdparanoia0:amd64.\n",
            "Preparing to unpack .../29-libcdparanoia0_3.10.2+debian-14build2_amd64.deb ...\n",
            "Unpacking libcdparanoia0:amd64 (3.10.2+debian-14build2) ...\n",
            "Selecting previously unselected package libvisual-0.4-0:amd64.\n",
            "Preparing to unpack .../30-libvisual-0.4-0_0.4.0-17build2_amd64.deb ...\n",
            "Unpacking libvisual-0.4-0:amd64 (0.4.0-17build2) ...\n",
            "Selecting previously unselected package gstreamer1.0-plugins-base:amd64.\n",
            "Preparing to unpack .../31-gstreamer1.0-plugins-base_1.20.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking gstreamer1.0-plugins-base:amd64 (1.20.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libgspell-1-common.\n",
            "Preparing to unpack .../32-libgspell-1-common_1.9.1-4_all.deb ...\n",
            "Unpacking libgspell-1-common (1.9.1-4) ...\n",
            "Selecting previously unselected package libgspell-1-2:amd64.\n",
            "Preparing to unpack .../33-libgspell-1-2_1.9.1-4_amd64.deb ...\n",
            "Unpacking libgspell-1-2:amd64 (1.9.1-4) ...\n",
            "Selecting previously unselected package libevview3-3:amd64.\n",
            "Preparing to unpack .../34-libevview3-3_42.3-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking libevview3-3:amd64 (42.3-0ubuntu3.1) ...\n",
            "Selecting previously unselected package gnome-desktop3-data.\n",
            "Preparing to unpack .../35-gnome-desktop3-data_42.9-0ubuntu1_all.deb ...\n",
            "Unpacking gnome-desktop3-data (42.9-0ubuntu1) ...\n",
            "Selecting previously unselected package libxkbregistry0:amd64.\n",
            "Preparing to unpack .../36-libxkbregistry0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbregistry0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgnome-desktop-3-19:amd64.\n",
            "Preparing to unpack .../37-libgnome-desktop-3-19_42.9-0ubuntu1_amd64.deb ...\n",
            "Unpacking libgnome-desktop-3-19:amd64 (42.9-0ubuntu1) ...\n",
            "Selecting previously unselected package libhandy-1-0:amd64.\n",
            "Preparing to unpack .../38-libhandy-1-0_1.6.1-1_amd64.deb ...\n",
            "Unpacking libhandy-1-0:amd64 (1.6.1-1) ...\n",
            "Selecting previously unselected package libnautilus-extension1a:amd64.\n",
            "Preparing to unpack .../39-libnautilus-extension1a_1%3a42.6-0ubuntu1_amd64.deb ...\n",
            "Unpacking libnautilus-extension1a:amd64 (1:42.6-0ubuntu1) ...\n",
            "Selecting previously unselected package libsecret-common.\n",
            "Preparing to unpack .../40-libsecret-common_0.20.5-2_all.deb ...\n",
            "Unpacking libsecret-common (0.20.5-2) ...\n",
            "Selecting previously unselected package libsecret-1-0:amd64.\n",
            "Preparing to unpack .../41-libsecret-1-0_0.20.5-2_amd64.deb ...\n",
            "Unpacking libsecret-1-0:amd64 (0.20.5-2) ...\n",
            "Selecting previously unselected package evince.\n",
            "Preparing to unpack .../42-evince_42.3-0ubuntu3.1_amd64.deb ...\n",
            "Unpacking evince (42.3-0ubuntu3.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../43-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package hmmer-doc.\n",
            "Preparing to unpack .../44-hmmer-doc_3.3.2+dfsg-1_all.deb ...\n",
            "Unpacking hmmer-doc (3.3.2+dfsg-1) ...\n",
            "Setting up bubblewrap (0.6.1-1) ...\n",
            "Setting up libtext-iconv-perl (1.7-7build3) ...\n",
            "Setting up libcdparanoia0:amd64 (3.10.2+debian-14build2) ...\n",
            "Setting up session-migration (0.3.6) ...\n",
            "Created symlink /etc/systemd/user/graphical-session-pre.target.wants/session-migration.service → /usr/lib/systemd/user/session-migration.service.\n",
            "Setting up libpoppler-glib8:amd64 (22.02.0-2ubuntu0.4) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libgspell-1-common (1.9.1-4) ...\n",
            "Setting up dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up evince-common (42.3-0ubuntu3.1) ...\n",
            "Setting up libvisual-0.4-0:amd64 (0.4.0-17build2) ...\n",
            "Setting up libaspell15:amd64 (0.60.8-4build1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up libnautilus-extension1a:amd64 (1:42.6-0ubuntu1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up gnome-desktop3-data (42.9-0ubuntu1) ...\n",
            "Setting up gstreamer1.0-plugins-base:amd64 (1.20.1-1ubuntu0.1) ...\n",
            "Setting up libxkbregistry0:amd64 (1.4.0-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up hmmer-doc (3.3.2+dfsg-1) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up aspell (0.60.8-4build1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up hunspell-en-us (1:2020.12.07-2) ...\n",
            "Setting up libgxps2:amd64 (0.3.2-2) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up libdjvulibre-text (3.5.28-2build2) ...\n",
            "Setting up libhunspell-1.7-0:amd64 (1.7.0-4build1) ...\n",
            "Setting up libsecret-common (0.20.5-2) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libhandy-1-0:amd64 (1.6.1-1) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.6) ...\n",
            "Setting up gsettings-desktop-schemas (42.0-1ubuntu1) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.6) ...\n",
            "Setting up libenchant-2-2:amd64 (2.3.2-1ubuntu2) ...\n",
            "Setting up libdjvulibre21:amd64 (3.5.28-2build2) ...\n",
            "Setting up aspell-en (2018.04.16-0-1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up enchant-2 (2.3.2-1ubuntu2) ...\n",
            "Setting up libspectre1:amd64 (0.2.10-1) ...\n",
            "Setting up libsecret-1-0:amd64 (0.20.5-2) ...\n",
            "Setting up libevdocument3-4:amd64 (42.3-0ubuntu3.1) ...\n",
            "Setting up libgspell-1-2:amd64 (1.9.1-4) ...\n",
            "Setting up libevview3-3:amd64 (42.3-0ubuntu3.1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libglib2.0-0:amd64 (2.72.4-0ubuntu2.3) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Setting up libgnome-desktop-3-19:amd64 (42.9-0ubuntu1) ...\n",
            "Setting up evince (42.3-0ubuntu3.1) ...\n",
            "Processing triggers for dictionaries-common (1.28.14) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78.)\n",
            "debconf: falling back to frontend: Readline\n",
            "aspell-autobuildhash: processing: en [en-common].\n",
            "aspell-autobuildhash: processing: en [en-variant_0].\n",
            "aspell-autobuildhash: processing: en [en-variant_1].\n",
            "aspell-autobuildhash: processing: en [en-variant_2].\n",
            "aspell-autobuildhash: processing: en [en-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_AU-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_AU-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_AU-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_CA-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_CA-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_CA-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ise-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-ize-wo_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_0].\n",
            "aspell-autobuildhash: processing: en [en_GB-variant_1].\n",
            "aspell-autobuildhash: processing: en [en_US-w_accents-only].\n",
            "aspell-autobuildhash: processing: en [en_US-wo_accents-only].\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  liblmdb0 ncbi-data\n",
            "The following NEW packages will be installed:\n",
            "  liblmdb0 ncbi-blast+ ncbi-data\n",
            "0 upgraded, 3 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 15.9 MB of archives.\n",
            "After this operation, 71.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblmdb0 amd64 0.9.24-1build2 [47.6 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ncbi-data all 6.1.20170106+dfsg1-9 [3,519 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 ncbi-blast+ amd64 2.12.0+ds-3build1 [12.3 MB]\n",
            "Fetched 15.9 MB in 1s (14.0 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package liblmdb0:amd64.\n",
            "(Reading database ... 126657 files and directories currently installed.)\n",
            "Preparing to unpack .../liblmdb0_0.9.24-1build2_amd64.deb ...\n",
            "Unpacking liblmdb0:amd64 (0.9.24-1build2) ...\n",
            "Selecting previously unselected package ncbi-data.\n",
            "Preparing to unpack .../ncbi-data_6.1.20170106+dfsg1-9_all.deb ...\n",
            "Unpacking ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Selecting previously unselected package ncbi-blast+.\n",
            "Preparing to unpack .../ncbi-blast+_2.12.0+ds-3build1_amd64.deb ...\n",
            "Unpacking ncbi-blast+ (2.12.0+ds-3build1) ...\n",
            "Setting up liblmdb0:amd64 (0.9.24-1build2) ...\n",
            "Setting up ncbi-data (6.1.20170106+dfsg1-9) ...\n",
            "Setting up ncbi-blast+ (2.12.0+ds-3build1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n"
          ]
        }
      ],
      "source": [
        "# install hmmer library\n",
        "!sudo apt-get install hmmer\n",
        "!sudo apt-get install hmmer-doc\n",
        "!sudo apt-get install ncbi-blast+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OlkE7gYYO4x9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the 3D structure\n",
        "CSV files of PF00014 domains are based on two different queries.\n",
        "\n",
        "```\n",
        "( Identifier = \"PF00014\" AND Annotation Type = \"Pfam\" ) AND Data Collection Resolution < 3 AND Polymer Entity Sequence Length = [ 50 - 80 ] AND Polymer Entity Mutation Count < 10\n",
        "```\n",
        "and\n",
        "```\n",
        " ( Identifier = \"PF00014\" AND Annotation Type = \"Pfam\" ) AND Data Collection Resolution < 2 AND Polymer Entity Sequence Length = [ 50 - 80 ] AND Polymer Entity Mutation Count < 2\n",
        " ```\n",
        " the difference are the `Resolution(3Å vs 2Å)`, `Polymer Entity Mutation Count(10 vs 2)` and at the end `grouping the polymer entities with different sequence identity(100% vs 50%)`.\n",
        "\n",
        " The stricter criteria, with a resolution of 2Å, a mutation count of less than 2, and a sequence identity of 100%, may lead to a smaller sample size but potentially higher quality data, while the less strict criteria, with a resolution of 3Å, a mutation count of less than 10, and a sequence identity of 50%, could yield a larger sample size but with a risk of including lower quality data. With the stricter rules, we have obtained 14 samples, whereas with the less strict rule, we have collected 28 samples. Additionally, we aim to assess which set of criteria—either the more stringent or the less restrictive—ultimately yields superior results in terms of sample quality and relevance to our research objectives."
      ],
      "metadata": {
        "id": "xvIignZ9RR3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can download the tabular CSV files which contain `Entity ID`, `Sequence`, `Auth Asym ID`"
      ],
      "metadata": {
        "id": "Xya4qG2nZlix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O strict_seq.csv \"https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/strict.csv\"\n",
        "!wget -O not_strict_seq.csv \"https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/not_strict.csv\""
      ],
      "metadata": {
        "id": "NWtEkVIDpPuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fee4140c-92fd-44c3-d037-c12c1bbeb176"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 19:29:53--  https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/strict.csv\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/strict.csv [following]\n",
            "--2024-05-16 19:29:53--  https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/strict.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1190 (1.2K) [text/plain]\n",
            "Saving to: ‘strict_seq.csv’\n",
            "\n",
            "strict_seq.csv      100%[===================>]   1.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-16 19:29:54 (83.8 MB/s) - ‘strict_seq.csv’ saved [1190/1190]\n",
            "\n",
            "--2024-05-16 19:29:54--  https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/not_strict.csv\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/not_strict.csv [following]\n",
            "--2024-05-16 19:29:54--  https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/not_strict.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2213 (2.2K) [text/plain]\n",
            "Saving to: ‘not_strict_seq.csv’\n",
            "\n",
            "not_strict_seq.csv  100%[===================>]   2.16K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-16 19:29:54 (33.6 MB/s) - ‘not_strict_seq.csv’ saved [2213/2213]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8uMuFACnusb"
      },
      "source": [
        "## Clean csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jiGaKijcO9Ry"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_csv_file(path: str, output_file_name: str, string_or_file: str = 'f', save_format: str = 'f') -> str or None:\n",
        "    \"\"\"\n",
        "    Reads and cleans a CSV file, providing options to return the cleaned data as a string or save it into a file.\n",
        "\n",
        "    Parameters:\n",
        "        path (str): The path to the CSV file to be cleaned.\n",
        "        output_file_name (str): The name of the output file. Defaults to \"output_seq\".\n",
        "        string_or_file (str): Determines whether to return the cleaned data as a string ('s') or save it into a file ('f'). Defaults to 'f'.\n",
        "        save_format (str): Determines the format for saving the data into a file. For keys ('k') or Fasta format ('f'). Defaults to 'f'.\n",
        "\n",
        "    Returns:\n",
        "        str or None: If the user chooses to get the results as a variable ('s'),\n",
        "        the cleaned data is returned as a string. If the user chooses to save the\n",
        "        results into a file ('f'), the cleaned data is saved into a file.\n",
        "    \"\"\"\n",
        "    print('-' * 40)\n",
        "    print(f'Reading CSV file from {path}...')\n",
        "    df = pd.read_csv(path)\n",
        "    print(f'Initial number of records: {len(df)}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Drop rows with missing 'Entity ID' and update 'Entity ID' format\n",
        "    df = df.dropna(subset=['Entity ID'])\n",
        "    print(f'Number of records after dropping missing Entity ID: {len(df)}')\n",
        "    df['Entity ID'] = df['Entity ID'].str.split('_').str[0] + ':' + df['Auth Asym ID']\n",
        "    df = df.drop(columns=['Auth Asym ID'])\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Option to return the cleaned data as a string\n",
        "    if string_or_file == 's':\n",
        "        cleaned_data = '\\n'.join(df['Entity ID'].values)\n",
        "        print('-' * 40)\n",
        "        print('Returning cleaned data as a string.')\n",
        "        return cleaned_data\n",
        "\n",
        "    # Option to save the cleaned data to a file\n",
        "    elif string_or_file == 'f':\n",
        "        if save_format == 'f':\n",
        "            output_path = output_file_name + '.fasta'\n",
        "            with open(output_path, 'w') as file:\n",
        "                for idx, row in df.iterrows():\n",
        "                    file.write(f\"> {row['Entity ID']}\\n{row['Sequence']}\\n\")\n",
        "            print(f'Data saved to {output_path}')\n",
        "        elif save_format == 'k':\n",
        "            output_path = output_file_name + '.txt'\n",
        "            with open(output_path, 'w') as f:\n",
        "                f.write('\\n'.join(df['Entity ID'].values))\n",
        "            print(f'Data saved to {output_path}')\n",
        "        print('-' * 40)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGiuiX3TPBpD",
        "outputId": "5b06a677-f0b7-4917-dfa9-619928a42056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading CSV file from strict_seq.csv...\n",
            "Initial number of records: 30\n",
            "----------------------------------------\n",
            "Number of records after dropping missing Entity ID: 15\n",
            "Data saved to strict_seqs.fasta\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "clean_csv_file(\n",
        "    path='strict_seq.csv',\n",
        "    output_file_name='strict_seqs',\n",
        "    string_or_file='f',\n",
        "    save_format='f'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_csv_file(\n",
        "    path='not_strict_seq.csv',\n",
        "    output_file_name='not_strict_seqs',\n",
        "    string_or_file='f',\n",
        "    save_format='f'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDvXKTaErmsF",
        "outputId": "f3e11c91-56f4-46ea-efd5-d864a1166893"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading CSV file from not_strict_seq.csv...\n",
            "Initial number of records: 49\n",
            "----------------------------------------\n",
            "Number of records after dropping missing Entity ID: 28\n",
            "Data saved to not_strict_seqs.fasta\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "HeHArpBRV69-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75715f94-2269-4cea-b2c5-65fc495062de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> 1AAP:A\n",
            "VREVCSEQAETGPCRAMISRWYFDVTEGKCAPFFYGGCGGNRNNFDTEEYCMAVCGSA\n",
            "> 1KTH:A\n",
            "ETDICKLPKDEGTCRDFILKWYYDPNTKSCARFWYGGCGGNENKFGSQKECEKVCAPV\n",
            "> 1ZR0:B\n",
            "PTGNNAEICLLPLDYGPCRALLLRYYYDRYTQSCRQFLYGGCEGNANNFYTWEACDDACWRIE\n",
            "> 3BYB:A\n",
            "KDRPDFCELPADTGPCRVRFPSFYYNPDEKKCLEFIYGGCEGNANNFITKEECESTCAA\n",
            "> 3M7Q:B\n",
            "EAEASICSEPKKVGRCKGYFPRFYFDSETGKCTPFIYGGCGGNGNNFETLHQCRAICRALG\n"
          ]
        }
      ],
      "source": [
        "!cat strict_seqs.fasta | head -n 10"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the same function which we used to clean the csv file, to only extract the ids of the sequences. these ids are going to be used as input files in the PDBeFold website to get Multiple Seqence Alignment based on Multiple Structure Alignemt."
      ],
      "metadata": {
        "id": "d1Neq1LVtnjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sequence ids as .txt file for the PDBeFold input\n",
        "clean_csv_file(\n",
        "    path='strict_seq.csv',\n",
        "    output_file_name='strict_ids',\n",
        "    string_or_file='f',\n",
        "    save_format='k'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Chk8dXA7svwC",
        "outputId": "2294ad3b-cd4c-4e69-ff4c-41a2a7d5f0d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading CSV file from strict_seq.csv...\n",
            "Initial number of records: 30\n",
            "----------------------------------------\n",
            "Number of records after dropping missing Entity ID: 15\n",
            "Data saved to strict_ids.txt\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get sequence ids as .txt file for the PDBeFold input\n",
        "clean_csv_file(\n",
        "    path='not_strict_seq.csv',\n",
        "    output_file_name='not_strict_ids',\n",
        "    string_or_file='f',\n",
        "    save_format='k'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whvLS6YetVPS",
        "outputId": "9c6a5102-9812-4160-f544-22a324e017a3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading CSV file from not_strict_seq.csv...\n",
            "Initial number of records: 49\n",
            "----------------------------------------\n",
            "Number of records after dropping missing Entity ID: 28\n",
            "Data saved to not_strict_ids.txt\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Files `strict_ids.txt` and `not_strict_ids.txt` are uploaded into the [PDBeFold](https://www.ebi.ac.uk/msd-srv/ssm/cgi-bin/ssmserver) and the results are saved in my Github repository."
      ],
      "metadata": {
        "id": "MKrKf9nqt8fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm not_strict_seq.csv strict_seq.csv"
      ],
      "metadata": {
        "id": "KozmCTcRr7i_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p171a3jTnzjb"
      },
      "source": [
        "## Get MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aQ6OxMmjgppZ",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a05e95f-9652-43e0-d7f9-62e31db687e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 19:29:55--  https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/strict_msa.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/strict_msa.txt [following]\n",
            "--2024-05-16 19:29:55--  https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/strict_msa.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1918 (1.9K) [text/plain]\n",
            "Saving to: ‘strict_msa.txt’\n",
            "\n",
            "strict_msa.txt      100%[===================>]   1.87K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-16 19:29:55 (25.6 MB/s) - ‘strict_msa.txt’ saved [1918/1918]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Getting the multiple sequence alignment from github repo\n",
        "!wget -O strict_msa.txt \"https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/strict_msa.txt\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O not_strict_msa.txt \"https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/not_strict_msa.txt\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "ytZUSAr8wn_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "790b3231-f9e0-42fb-c9b7-6350b0d73862"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 19:29:56--  https://github.com/heispv/bioinformatics/raw/master/lab-of-bioinformatics/project_files/not_strict_msa.txt\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/not_strict_msa.txt [following]\n",
            "--2024-05-16 19:29:56--  https://raw.githubusercontent.com/heispv/bioinformatics/master/lab-of-bioinformatics/project_files/not_strict_msa.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4029 (3.9K) [text/plain]\n",
            "Saving to: ‘not_strict_msa.txt’\n",
            "\n",
            "not_strict_msa.txt  100%[===================>]   3.93K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-05-16 19:29:56 (44.9 MB/s) - ‘not_strict_msa.txt’ saved [4029/4029]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm strict_ids.txt not_strict_ids.txt"
      ],
      "metadata": {
        "id": "GN6awguTz5yq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDphtcV1n3c8"
      },
      "source": [
        "### Build HMM based on the raw MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "I2fV858OjkKf",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d819b7cc-3fd8-438a-f8d8-89e7e5a6d205"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
            "# HMMER 3.3.2 (Nov 2020); http://hmmer.org/\n",
            "# Copyright (C) 2020 Howard Hughes Medical Institute.\n",
            "# Freely distributed under the BSD open source license.\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "# input alignment file:             strict_msa.txt\n",
            "# output HMM file:                  strict_msa_not_clean.hmm\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
            "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
            "1     strict_msa              15    63    57     2.35  0.976 \n",
            "\n",
            "# CPU time: 0.05u 0.00s 00:00:00.05 Elapsed: 00:00:00.07\n"
          ]
        }
      ],
      "source": [
        "# Create an HMM model based on the strict_msa.txt file\n",
        "!hmmbuild strict_msa_not_clean.hmm strict_msa.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an HMM model based on the not_strict_msa.txt file\n",
        "!hmmbuild not_strict_msa_not_clean.hmm not_strict_msa.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fW3sc21hxLFK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba96742f-e8fe-4515-92c1-4b1a6f8f4e78"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
            "# HMMER 3.3.2 (Nov 2020); http://hmmer.org/\n",
            "# Copyright (C) 2020 Howard Hughes Medical Institute.\n",
            "# Freely distributed under the BSD open source license.\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "# input alignment file:             not_strict_msa.txt\n",
            "# output HMM file:                  not_strict_msa_not_clean.hmm\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
            "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
            "1     not_strict_msa          28    80    59     3.28  0.945 \n",
            "\n",
            "# CPU time: 0.05u 0.00s 00:00:00.05 Elapsed: 00:00:00.05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hbLbFy9Gk0r6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f51459-255e-43fe-b5cc-6ce0408f8a41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMMER3/f [3.3.2 | Nov 2020]\n",
            "NAME  strict_msa\n",
            "LENG  57\n",
            "ALPH  amino\n",
            "RF    no\n",
            "MM    no\n",
            "CONS  yes\n",
            "CS    no\n",
            "MAP   yes\n",
            "DATE  Thu May 16 19:29:56 2024\n",
            "NSEQ  15\n",
            "EFFN  2.354736\n",
            "CKSUM 1225978556\n",
            "STATS LOCAL MSV       -8.8676  0.71902\n",
            "STATS LOCAL VITERBI   -9.0620  0.71902\n",
            "STATS LOCAL FORWARD   -4.0485  0.71902\n",
            "HMM          A        C        D        E        F        G        H        I        K        L        M        N        P        Q        R        S        T        V        W        Y   \n",
            "            m->m     m->i     m->d     i->m     i->i     d->m     d->d\n",
            "  COMPO   2.60263  2.73612  3.07114  2.69296  2.83763  2.63193  3.88604  3.41319  2.66892  3.03622  4.07894  2.70680  3.44272  3.07560  2.87241  2.77771  2.91062  3.13755  4.62504  2.85507\n",
            "          2.68622  4.42249  2.77475  2.73061  3.46378  2.40519  3.72518  3.29307  2.67748  2.69379  4.24714  2.90341  2.73739  3.18170  2.89777  2.37911  2.77518  2.98542  4.58501  3.61527\n",
            "          0.72401  0.99536  1.92687  0.72737  0.66006  0.00000        *\n",
            "      1   2.79049  5.22316  3.04887  2.12469  4.55881  3.55397  3.72323  4.01726  2.19577  3.51194  4.27356  2.66965  2.92656  2.83034  1.74342  2.76327  3.01677  3.12779  5.65072  4.27566      5 r - - -\n"
          ]
        }
      ],
      "source": [
        "!cat strict_msa_not_clean.hmm | head -n 22"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat not_strict_msa_not_clean.hmm | head -n 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2JGZL3axfmS",
        "outputId": "42e9a0ad-6422-4c36-8e7f-e8f99b383957"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMMER3/f [3.3.2 | Nov 2020]\n",
            "NAME  not_strict_msa\n",
            "LENG  59\n",
            "ALPH  amino\n",
            "RF    no\n",
            "MM    no\n",
            "CONS  yes\n",
            "CS    no\n",
            "MAP   yes\n",
            "DATE  Thu May 16 19:29:56 2024\n",
            "NSEQ  28\n",
            "EFFN  3.284668\n",
            "CKSUM 1929399890\n",
            "STATS LOCAL MSV       -8.9836  0.71896\n",
            "STATS LOCAL VITERBI   -9.1716  0.71896\n",
            "STATS LOCAL FORWARD   -4.2058  0.71896\n",
            "HMM          A        C        D        E        F        G        H        I        K        L        M        N        P        Q        R        S        T        V        W        Y   \n",
            "            m->m     m->i     m->d     i->m     i->i     d->m     d->d\n",
            "  COMPO   2.66045  2.92817  3.06444  2.66755  2.75405  2.64152  3.84570  3.40036  2.59333  3.03034  4.07046  2.68786  3.42610  3.06466  2.78686  2.80020  2.93294  3.20393  4.59062  2.86329\n",
            "          2.68661  4.42268  2.77530  2.73007  3.46397  2.40541  3.72365  3.29271  2.67756  2.69368  4.24660  2.90390  2.73730  3.18114  2.89805  2.37919  2.77491  2.98562  4.58371  3.61546\n",
            "          1.18984  1.11301  1.00195  1.14078  0.38503  0.00000        *\n",
            "      1   2.54629  5.21851  2.35429  2.29839  4.55912  3.48998  3.67937  4.04122  2.16020  3.16993  4.26098  2.68158  3.23191  2.24746  2.44821  2.68367  2.95102  3.60688  5.65277  4.24542     20 k - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2teyVJNOljwu"
      },
      "source": [
        "Based on the files above, we can observe that the `hmmbuild` command, applied to the `strict_msa.txt` file, cuts the first `5` characters in the sequence while when using the `not_strict_msa.txt` it cuts the first `20` character. This action is taken because there are not enough amino acids to build the Hidden Markov Model (HMM) for that part of the sequence. Therefore, we will trim each sequence and then reapply the `hmmbuild` command."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tV4yFT9n9Gb"
      },
      "source": [
        "## Clean raw MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3jwmyr4ShxgI"
      },
      "outputs": [],
      "source": [
        "def clean_msa(path: str, first_clipping_num: int, output_file_name: str) -> None:\n",
        "    \"\"\"\n",
        "    Clean MSA file by removing specified number of characters from the beginning of each sequence.\n",
        "\n",
        "    Args:\n",
        "        path (str): Path to the input MSA file.\n",
        "        first_clipping_num (int): Number of characters to remove from the beginning of each sequence.\n",
        "        output_file_name (str): Name of the output file.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "\n",
        "    This function reads a MSA file, extracts the sequence IDs and sequences, removes the specified\n",
        "    number of characters from the beginning of each sequence, and writes the cleaned sequences to a new file.\n",
        "    \"\"\"\n",
        "    print('-' * 40)\n",
        "    print(f'Reading MSA file from {path}...')\n",
        "    with open(path) as f:\n",
        "        fastas = f.read().strip().split('\\n\\n')\n",
        "    print(f'Number of sequences found: {len(fastas)}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    clean_list = []\n",
        "    for fasta in fastas:\n",
        "        id = fasta.split()[0]\n",
        "        sequence = ''.join(fasta.split('\\n')[1:])\n",
        "        clean_list.append((id, sequence))\n",
        "\n",
        "    print(f'Removing the first {first_clipping_num} characters from each sequence...')\n",
        "    with open(output_file_name + '.txt', 'w') as f:\n",
        "        for item in clean_list:\n",
        "            f.write(f\"{item[0]}\\n{item[1][first_clipping_num:]}\\n\")\n",
        "    print('-' * 40)\n",
        "\n",
        "    print(f'Output saved in {output_file_name}.txt')\n",
        "    print('-' * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zIxZVxctiuub",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1bae80b-12e4-4802-98ce-58a643191cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading MSA file from strict_msa.txt...\n",
            "Number of sequences found: 15\n",
            "----------------------------------------\n",
            "Removing the first 5 characters from each sequence...\n",
            "----------------------------------------\n",
            "Output saved in clean_strict_msa.txt\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "clean_msa(\n",
        "    path='strict_msa.txt',\n",
        "    first_clipping_num=5,\n",
        "    output_file_name='clean_strict_msa'\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_msa(\n",
        "    path='not_strict_msa.txt',\n",
        "    first_clipping_num=20,\n",
        "    output_file_name='clean_not_strict_msa'\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82626ISayc_y",
        "outputId": "2e50c2c6-ee71-40ef-e79e-a1dd0bf281f9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading MSA file from not_strict_msa.txt...\n",
            "Number of sequences found: 28\n",
            "----------------------------------------\n",
            "Removing the first 20 characters from each sequence...\n",
            "----------------------------------------\n",
            "Output saved in clean_not_strict_msa.txt\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "HbO7jn3njnD-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39da6c2b-eb9e-4295-ef85-6f5c7fed06b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">PDB:1aap:A\n",
            "revcseqaetgpcrAMISRWYFDVTEGKCAPFFYGGCGGNRNNFDTEEYCMAVCg---\n",
            ">PDB:1kth:A\n",
            "tdicklpkdegtcrDFILKWYYDPNTKSCARFWYGGCGGNENKFGSQKECEKVCapv-\n",
            ">PDB:1zr0:B\n",
            "aeicllpldygpcrALLLRYYYDRYTQSCRQFLYGGCEGNANNFYTWEACDDACwrie\n",
            ">PDB:3byb:A\n",
            "pdfcelpadtgpcrVRFPSFYYNPDEKKCLEFIYGGCEGNANNFITKEECESTCa---\n",
            ">PDB:3m7q:B\n",
            "asicsepkkvgrckGYFPRFYFDSETGKCTPFIYGGCGGNGNNFETLHQCRAICralg\n",
            ">PDB:3wny:C\n",
            "pafcleppyagpgkARIIRYFYNAKAGAAQAFVYGGVRAKRNNFASAADALAACaa--\n",
            ">PDB:4dtg:K\n",
            "pdfcfleedpgicrGYITRYFYNNQTKQCERFKYGGCLGNMNNFETLEECKNICedgh\n",
            ">PDB:4ntw:B\n",
            "afcyedppffqkcgAFVDSYYFNRSRITCVHFFYGQCDVNQNHFTTMSECNRVChg--\n",
            ">PDB:4u30:X\n",
            "-acanlpivrgpcrAFIQLWAFDAVKGKCVLFPYGGCQGNGNKFYSEKECREYCg---\n",
            ">PDB:4u32:X\n",
            "hdfclvskvvgrcrASMPRWWYNVTDGSCQLFVYGGCDGNSNNYLTKEECLKKC----\n"
          ]
        }
      ],
      "source": [
        "# Check the strict_msa.tx file\n",
        "!cat clean_strict_msa.txt | head -n 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EBfaMw91i8Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90f5c9df-4b30-49ef-f7c5-8704a4fe8f91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">PDB:1aap:A\n",
            "vrevcseqaetgpcrAMISRWYFDVTEGKCAPFFYGGcGG-NRNNFDTEEYCMAVCg---\n",
            ">PDB:1bun:B\n",
            "rhpdcdkppdtkicqTVVRAFYYKPSAKRCVQFRYGG-CNgNGNHFKSDHLCRCECleyr\n",
            ">PDB:1dtx:A\n",
            "rrklcilhrnpgrcyDKIPAFYYNQKKKQCERFDWSGcGG-NSNRFKTIEECRRTCig--\n",
            ">PDB:1fak:I\n",
            "apdfcleppydgpcrALHLRYFYNAKAGLCQTFYYGGcLA-KRNNFESAEDCMRTC----\n",
            ">PDB:1g6x:A\n",
            "rpdfcleppyagacrARIIRYFYNAKAGLCQTFVYGGcRA-KRNNFKSAEDCLRTCgga-\n",
            ">PDB:1kth:A\n",
            "etdicklpkdegtcrDFILKWYYDPNTKSCARFWYGGcGG-NENKFGSQKECEKVCapv-\n",
            ">PDB:1tfx:C\n",
            "kpdfcfleedpgicrGYITRYFYNNQTKQCERFKYGGcLG-NMNNFETLEECKNICedg-\n",
            ">PDB:1yc0:I\n",
            "tedyclasnkvgrcrGSFPRWYYDPTEQICKSFVYGGcLG-NKNNYLREEECILACrgv-\n",
            ">PDB:1ylc:B\n",
            "rpdfxleppytgpckARIIRYFYNAKAGLXQTFVYGGcRA-KRNNFKSAEDXMRTXg---\n",
            ">PDB:1yld:B\n",
            "rpdfxleppytgpckARIIRYFYNAPDGLXQTFVYGGcRA-KRNNFKSAEDXMRTXg---\n"
          ]
        }
      ],
      "source": [
        "# Check the not_strict_msa.tx file\n",
        "!cat clean_not_strict_msa.txt | head -n 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm not_strict_msa.txt not_strict_msa_not_clean.hmm strict_msa.txt strict_msa_not_clean.hmm"
      ],
      "metadata": {
        "id": "kya1I8aE42Uy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYFlGEFMoA87"
      },
      "source": [
        "### Build HMM based on clean MSA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2VXkmCYDndvf",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead9540f-a7ce-4fbe-b799-119f67acad94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
            "# HMMER 3.3.2 (Nov 2020); http://hmmer.org/\n",
            "# Copyright (C) 2020 Howard Hughes Medical Institute.\n",
            "# Freely distributed under the BSD open source license.\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "# input alignment file:             clean_strict_msa.txt\n",
            "# output HMM file:                  strict_msa.hmm\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
            "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
            "1     clean_strict_msa        15    58    56     2.38  0.994 \n",
            "\n",
            "# CPU time: 0.04u 0.00s 00:00:00.04 Elapsed: 00:00:00.05\n"
          ]
        }
      ],
      "source": [
        "# Create an HMM model based on the clean_strict_msa.txt file\n",
        "!hmmbuild strict_msa.hmm clean_strict_msa.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an HMM model based on the clean_not_strict_msa.txt file\n",
        "!hmmbuild not_strict_msa.hmm clean_not_strict_msa.txt"
      ],
      "metadata": {
        "collapsed": true,
        "id": "57XIiA5P5Jpi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f599f25-b2e2-413c-e2f5-d372f8890a95"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# hmmbuild :: profile HMM construction from multiple sequence alignments\n",
            "# HMMER 3.3.2 (Nov 2020); http://hmmer.org/\n",
            "# Copyright (C) 2020 Howard Hughes Medical Institute.\n",
            "# Freely distributed under the BSD open source license.\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "# input alignment file:             clean_not_strict_msa.txt\n",
            "# output HMM file:                  not_strict_msa.hmm\n",
            "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
            "\n",
            "# idx name                  nseq  alen  mlen eff_nseq re/pos description\n",
            "#---- -------------------- ----- ----- ----- -------- ------ -----------\n",
            "1     clean_not_strict_msa    28    60    58     3.31  0.961 \n",
            "\n",
            "# CPU time: 0.04u 0.00s 00:00:00.04 Elapsed: 00:00:00.06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "L4sSoDTPk8JF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccec17bf-4f9b-43bc-f889-44da10fe1693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMMER3/f [3.3.2 | Nov 2020]\n",
            "NAME  clean_strict_msa\n",
            "LENG  56\n",
            "ALPH  amino\n",
            "RF    no\n",
            "MM    no\n",
            "CONS  yes\n",
            "CS    no\n",
            "MAP   yes\n",
            "DATE  Thu May 16 19:29:57 2024\n",
            "NSEQ  15\n",
            "EFFN  2.384033\n",
            "CKSUM 2703071005\n",
            "STATS LOCAL MSV       -8.8221  0.71901\n",
            "STATS LOCAL VITERBI   -9.0502  0.71901\n",
            "STATS LOCAL FORWARD   -4.0496  0.71901\n",
            "HMM          A        C        D        E        F        G        H        I        K        L        M        N        P        Q        R        S        T        V        W        Y   \n",
            "            m->m     m->i     m->d     i->m     i->i     d->m     d->d\n",
            "  COMPO   2.59850  2.70859  3.07791  2.70650  2.81735  2.62294  3.89445  3.41061  2.67810  3.03829  4.07877  2.70340  3.46464  3.08028  2.90361  2.78551  2.91215  3.14231  4.61766  2.83458\n",
            "          2.68618  4.42225  2.77519  2.73123  3.46354  2.40513  3.72494  3.29354  2.67741  2.69355  4.24690  2.90347  2.73739  3.18146  2.89801  2.37887  2.77519  2.98518  4.58477  3.61503\n",
            "          0.10417  4.75186  2.40473  0.61958  0.77255  0.00000        *\n",
            "      1   1.96157  4.93359  3.22395  2.69275  4.27031  3.57957  3.07490  3.68556  2.64164  3.28965  4.10709  3.18837  1.77260  3.02513  2.74856  2.82675  2.66477  3.34745  5.52693  4.20723      1 p - - -\n"
          ]
        }
      ],
      "source": [
        "!cat strict_msa.hmm | head -n 22"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat not_strict_msa.hmm | head -n 22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-LHcJsS5WQh",
        "outputId": "5438616f-83f8-4cee-d1ca-34f6f7b013a3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HMMER3/f [3.3.2 | Nov 2020]\n",
            "NAME  clean_not_strict_msa\n",
            "LENG  58\n",
            "ALPH  amino\n",
            "RF    no\n",
            "MM    no\n",
            "CONS  yes\n",
            "CS    no\n",
            "MAP   yes\n",
            "DATE  Thu May 16 19:29:57 2024\n",
            "NSEQ  28\n",
            "EFFN  3.312012\n",
            "CKSUM 1133647359\n",
            "STATS LOCAL MSV       -8.9546  0.71898\n",
            "STATS LOCAL VITERBI   -9.1394  0.71898\n",
            "STATS LOCAL FORWARD   -4.2119  0.71898\n",
            "HMM          A        C        D        E        F        G        H        I        K        L        M        N        P        Q        R        S        T        V        W        Y   \n",
            "            m->m     m->i     m->d     i->m     i->i     d->m     d->d\n",
            "  COMPO   2.66190  2.90256  3.08226  2.67195  2.73539  2.63859  3.85121  3.39815  2.59820  3.03714  4.06723  2.68399  3.44414  3.07780  2.78809  2.81204  2.93638  3.20605  4.58366  2.84542\n",
            "          2.68618  4.42225  2.77519  2.73123  3.46354  2.40513  3.72494  3.29354  2.67741  2.69355  4.24690  2.90347  2.73739  3.18146  2.89801  2.37887  2.77519  2.98518  4.58477  3.61503\n",
            "          0.12720  5.00516  2.18269  0.61958  0.77255  0.00000        *\n",
            "      1   2.70081  5.26156  3.12664  2.27441  4.57378  3.62725  3.81500  4.03441  2.33418  3.55118  3.35474  2.82205  2.76859  2.92409  1.67611  2.82092  2.67473  3.33076  5.71322  4.32722      1 r - - -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1AhdpqJlU1r"
      },
      "source": [
        "* In these new files, we can observe that the probabilities start from the first amino acid (AA), indicating that no cutting is performed by the `hmmbuild` command itself."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Works only on my own Drive\n",
        "# !cp /content/drive/MyDrive/lab_of_bioinformatics/negative.fasta /content/negative.fasta"
      ],
      "metadata": {
        "id": "4jMMYjaRjY0l"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt_2QpJumlhS"
      },
      "source": [
        "## Get the negative and postive data from NCBI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tk_KQIkVoLVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f43e810-a955-4f54-841a-782f2033fc54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-13 19:39:45--  https://rest.uniprot.org/uniprotkb/stream?compressed=true&format=fasta&query=%28%28reviewed%3Atrue%29+NOT+%28xref%3Apfam-PF00014%29%29\n",
            "Resolving rest.uniprot.org (rest.uniprot.org)... 193.62.193.81\n",
            "Connecting to rest.uniprot.org (rest.uniprot.org)|193.62.193.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘negative.fasta.gz’\n",
            "\n",
            "negative.fasta.gz       [        <=>         ] 134.34M   159KB/s    in 14m 47s \n",
            "\n",
            "2024-05-13 19:54:33 (155 KB/s) - ‘negative.fasta.gz’ saved [140864598]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O negative.fasta.gz \"https://rest.uniprot.org/uniprotkb/stream?compressed=true&format=fasta&query=%28%28reviewed%3Atrue%29+NOT+%28xref%3Apfam-PF00014%29%29\"\n",
        "!zcat -f negative.fasta.gz > negative.fasta\n",
        "!rm negative.fasta.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "jA0KSmhMoTsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b58f5456-912b-4e28-a211-7f4169009667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-16 19:30:05--  https://rest.uniprot.org/uniprotkb/stream?compressed=true&format=fasta&query=%28%28xref%3Apfam-PF00014%29+AND+%28reviewed%3Atrue%29%29\n",
            "Resolving rest.uniprot.org (rest.uniprot.org)... 193.62.193.81\n",
            "Connecting to rest.uniprot.org (rest.uniprot.org)|193.62.193.81|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 \n",
            "Length: unspecified [text/plain]\n",
            "Saving to: ‘bpti_reviewd.fasta.gz’\n",
            "\n",
            "bpti_reviewd.fasta.     [  <=>               ]  41.28K   152KB/s    in 0.3s    \n",
            "\n",
            "2024-05-16 19:30:06 (152 KB/s) - ‘bpti_reviewd.fasta.gz’ saved [42270]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# It contains the human and non-human kunitz domain\n",
        "!wget -O bpti_reviewd.fasta.gz \"https://rest.uniprot.org/uniprotkb/stream?compressed=true&format=fasta&query=%28%28xref%3Apfam-PF00014%29+AND+%28reviewed%3Atrue%29%29\"\n",
        "!zcat bpti_reviewd.fasta.gz > bpti_reviewd.fasta\n",
        "!rm bpti_reviewd.fasta.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "351B7ev0qary",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "39521052-4087-425a-ee05-3a7e03cd04d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Building a new DB, current time: 05/16/2024 19:30:06\n",
            "New DB name:   /content/strict_seqs.fasta\n",
            "New DB title:  strict_seqs.fasta\n",
            "Sequence type: Protein\n",
            "Keep MBits: T\n",
            "Maximum file size: 1000000000B\n",
            "Adding sequences from FASTA; added 15 sequences in 0.00117588 seconds.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Building a new DB, current time: 05/16/2024 19:30:07\n",
            "New DB name:   /content/not_strict_seqs.fasta\n",
            "New DB title:  not_strict_seqs.fasta\n",
            "Sequence type: Protein\n",
            "Keep MBits: T\n",
            "Maximum file size: 1000000000B\n",
            "Adding sequences from FASTA; added 28 sequences in 0.00164604 seconds.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Make blast dataset for the strict_seqs.fasta and not_strict_seqs.fasta\n",
        "!makeblastdb -in strict_seqs.fasta -dbtype prot\n",
        "!makeblastdb -in not_strict_seqs.fasta -dbtype prot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat bpti_reviewd.fasta | grep \">\" | wc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpo4Qv8XRAOJ",
        "outputId": "100d2cf4-6fee-4b50-8758-c9502b01ea1c"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    391    3918   40444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This command below filters a BLAST result file named \"bpti_reviewd.blast\". It removes comment lines (lines starting with # character), selects entries with sequence identity greater than 98%, and saves the unique identifiers of those entries into a file named \"remove.fasta\"."
      ],
      "metadata": {
        "id": "-buKcNfgGmDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v \"^#\" strict_bpti.blast | awk '{if ($3 > 98) {print $0}}' | cut -f 1 | sort -u > strict_remove_98.ids\n",
        "!grep -v \"^#\" not_strict_bpti.blast | awk '{if ($3 > 98) {print $0}}' | cut -f 1 | sort -u > not_strict_remove_98.ids"
      ],
      "metadata": {
        "id": "DxqE2HOEF-of",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a8e8d98-09b5-4b9f-a129-51c1d609f5f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grep: strict_bpti.blast: No such file or directory\n",
            "grep: not_strict_bpti.blast: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v \"^#\" strict_bpti.blast | awk '{if ($3 > 80) {print $0}}' | cut -f 1 | sort -u > strict_remove_80.ids\n",
        "!grep -v \"^#\" not_strict_bpti.blast | awk '{if ($3 > 80) {print $0}}' | cut -f 1 | sort -u > not_strict_remove_80.ids"
      ],
      "metadata": {
        "id": "Qdn1Y5Zqmr4S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bfe7e80-28b8-488a-f91a-eae7ebaf0d56"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grep: strict_bpti.blast: No such file or directory\n",
            "grep: not_strict_bpti.blast: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat strict_remove_98.ids | wc\n",
        "!cat not_strict_remove_98.ids | wc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s-1lVuvk4yO",
        "outputId": "e75dfe92-a774-421d-8b5f-318c2c3b1549"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0       0       0\n",
            "      0       0       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat strict_remove_80.ids | wc\n",
        "!cat not_strict_remove_80.ids | wc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjRY9sLjm3ce",
        "outputId": "4155e029-2aa3-4c40-b1c6-35005911731a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      0       0       0\n",
            "      0       0       0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat strict_remove_80.ids | head -n 5"
      ],
      "metadata": {
        "id": "yfzPq6auG6pA"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are only interested in the ids, to get the ids from the remove.fasta file, we should run the command below. the results would be saved in the `remove.ids` file."
      ],
      "metadata": {
        "id": "8OnsnACiHEmY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the `remove.ids` file, there are 27 sequences which should be removed from the main data."
      ],
      "metadata": {
        "id": "bnU7sqOTIFaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_sequences(seq_file_path, ids_file_path, output_file_path):\n",
        "    \"\"\"\n",
        "    Filters sequences from a FASTA file based on a list of excluded sequence IDs and saves them in a file.\n",
        "\n",
        "    Parameters:\n",
        "    - seq_file_path (str): The file path to the input FASTA file containing sequences to filter.\n",
        "    - ids_file_path (str): The file path to the input file containing a list of sequence IDs to exclude.\n",
        "    - output_file_path (str): The file path to save the filtered sequences.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "    print('-' * 40)\n",
        "    print(f'Reading excluded sequence IDs from {ids_file_path}...')\n",
        "    # Open the file containing excluded sequence IDs and create a set to store them\n",
        "    with open(ids_file_path, 'r') as f:\n",
        "        excluded_ids = [line.strip().split('|')[1] for line in f]\n",
        "    print(f'Number of excluded IDs: {len(excluded_ids)}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    print(f'Reading sequences from {seq_file_path}...')\n",
        "    # Open the input FASTA file and extract sequences\n",
        "    with open(seq_file_path, 'r') as f:\n",
        "        content = f.read().strip()\n",
        "        sequences = content.split('>')[1:]\n",
        "    print(f'Number of sequences found: {len(sequences)}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    print(f'Filtering sequences and writing to {output_file_path}...')\n",
        "    # Open the output file for writing filtered sequences\n",
        "    with open(output_file_path, 'w') as outfile:\n",
        "        filtered_count = 0\n",
        "        for sequence in sequences:\n",
        "            header = sequence.split('\\n', 1)[0]\n",
        "            seq_id = header.split('|')[1]\n",
        "\n",
        "            if seq_id not in excluded_ids:\n",
        "                outfile.write(f'>{sequence}\\n')\n",
        "                filtered_count += 1\n",
        "    print(f'Number of sequences written to output: {filtered_count}')\n",
        "    print('-' * 40)\n",
        "    print('Filtering process completed.')\n"
      ],
      "metadata": {
        "id": "5nKvR4QdIOGg"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sequences('bpti_reviewd.fasta', 'strict_remove_98.ids', 'strict_pos_98.fasta')"
      ],
      "metadata": {
        "id": "krKplA0-Ilkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71be4d82-2b32-4f7b-f279-65a03375621f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading excluded sequence IDs from strict_remove_98.ids...\n",
            "Number of excluded IDs: 0\n",
            "----------------------------------------\n",
            "Reading sequences from bpti_reviewd.fasta...\n",
            "Number of sequences found: 391\n",
            "----------------------------------------\n",
            "Filtering sequences and writing to strict_pos_98.fasta...\n",
            "Number of sequences written to output: 391\n",
            "----------------------------------------\n",
            "Filtering process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sequences('bpti_reviewd.fasta', 'strict_remove_80.ids', 'strict_pos_80.fasta')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ANFFekNtSvV",
        "outputId": "274b2238-2bbf-4dd4-9f67-f08857463cb7"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading excluded sequence IDs from strict_remove_80.ids...\n",
            "Number of excluded IDs: 0\n",
            "----------------------------------------\n",
            "Reading sequences from bpti_reviewd.fasta...\n",
            "Number of sequences found: 391\n",
            "----------------------------------------\n",
            "Filtering sequences and writing to strict_pos_80.fasta...\n",
            "Number of sequences written to output: 391\n",
            "----------------------------------------\n",
            "Filtering process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sequences('bpti_reviewd.fasta', 'not_strict_remove_98.ids', 'not_strict_pos_98.fasta')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ty45pujRtUM6",
        "outputId": "355b4b2c-e7b0-4a05-9fed-ffb8bd36caf6"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading excluded sequence IDs from not_strict_remove_98.ids...\n",
            "Number of excluded IDs: 0\n",
            "----------------------------------------\n",
            "Reading sequences from bpti_reviewd.fasta...\n",
            "Number of sequences found: 391\n",
            "----------------------------------------\n",
            "Filtering sequences and writing to not_strict_pos_98.fasta...\n",
            "Number of sequences written to output: 391\n",
            "----------------------------------------\n",
            "Filtering process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filter_sequences('bpti_reviewd.fasta', 'not_strict_remove_80.ids', 'not_strict_pos_80.fasta')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiuQNgHTtV28",
        "outputId": "2480d6e1-fa93-46de-8b6b-4f3411c316b7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Reading excluded sequence IDs from not_strict_remove_80.ids...\n",
            "Number of excluded IDs: 0\n",
            "----------------------------------------\n",
            "Reading sequences from bpti_reviewd.fasta...\n",
            "Number of sequences found: 391\n",
            "----------------------------------------\n",
            "Filtering sequences and writing to not_strict_pos_80.fasta...\n",
            "Number of sequences written to output: 391\n",
            "----------------------------------------\n",
            "Filtering process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list = ['strict_pos_98.fasta', 'strict_pos_80.fasta', 'not_strict_pos_98.fasta', 'not_strict_pos_80.fasta']"
      ],
      "metadata": {
        "id": "nZwYNBAepGVG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat strict_pos_98.fasta | head -n 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY5WK2y7k2vs",
        "outputId": "fab622b2-1e0b-40cb-f2cc-00f89eda4dc6"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">sp|A0A1Z0YU59|MAMB1_DENAN Mambaquaretin-1 OS=Dendroaspis angusticeps OX=8618 PE=1 SV=2\n",
            "RPSFCNLPVKPGPCNGFFSAFYYSQKTNKCHSFTYGGCKGNANRFSTIEKCRRTCVG\n",
            "\n",
            ">sp|B2G331|VKT2B_HETCR TauPI-stichotoxin-Hcr2b OS=Heteractis crispa OX=175771 PE=1 SV=1\n",
            "MKGTFLICLILIAGFSFKSTQAGSICLEPKVVGPCTAYFRRFYFDSETGKCTVFIYGGCE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to have a simplified version of these fasta files..."
      ],
      "metadata": {
        "id": "x1gvYJw1k8Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simplify_fasta(file_path):\n",
        "    # Read the input file\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = f.read()\n",
        "\n",
        "    simplified = []\n",
        "    entries = [entry for entry in data.split('>sp') if entry.strip()]\n",
        "\n",
        "    for entry in entries:\n",
        "        lines = entry.split('\\n')\n",
        "        # Check if '|' is in the first line and if not, handle gracefully\n",
        "        identifier = lines[0].split('|')[1] if '|' in lines[0] else lines[0]\n",
        "\n",
        "        # Join all lines except the first to get the sequence, and remove new lines\n",
        "        sequence = ''.join(lines[1:]).replace('\\n', '')\n",
        "        simplified_entry = f'>{identifier}\\n{sequence}'\n",
        "        simplified.append(simplified_entry)\n",
        "\n",
        "    # Create the output content\n",
        "    output_content = '\\n\\n'.join(simplified)\n",
        "\n",
        "    # Create the output file name\n",
        "    output_file_path = file_path.rsplit('.', 1)[0] + '_simple.' + file_path.rsplit('.', 1)[1]\n",
        "\n",
        "    # Write the simplified content to the new file\n",
        "    with open(output_file_path, 'w') as output_file:\n",
        "        output_file.write(output_content)\n",
        "\n",
        "    print(f'File saved as {output_file_path}')\n",
        "    return output_file_path  # Return the output file path"
      ],
      "metadata": {
        "id": "zwZZ8HD0k7sc"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simplify_fasta(\"strict_pos_80.fasta\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7aaMFkHAlLBM",
        "outputId": "802a076d-8341-4c1e-8ea6-7b23ca3d7718"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File saved as strict_pos_80_simple.fasta\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'strict_pos_80_simple.fasta'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_fasta(data):\n",
        "    import random\n",
        "    # Split the data into entries using a marker that identifies new entries starting with '>'\n",
        "    # This time, we split directly on the '>' and preserve it in the next step\n",
        "    entries = data.strip().split('\\n>')\n",
        "\n",
        "    # Attach '>' back to each entry except the first one if it was removed\n",
        "    entries = [entry if entry.startswith('>') else '>' + entry for entry in entries]\n",
        "\n",
        "    # Shuffle the entries list randomly\n",
        "    random.shuffle(entries)\n",
        "\n",
        "    # Join all shuffled entries with a single newline between them, after ensuring each ends cleanly\n",
        "    return '\\n\\n'.join(entry.strip() for entry in entries)"
      ],
      "metadata": {
        "id": "ywwZRwB9nAp8"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_fasta_files(positive_path, negative_path, training_portion):\n",
        "    # Simplify the data\n",
        "    print('-' * 40)\n",
        "    print(f'Simplifying the data for {positive_path} ...')\n",
        "    positive_simplified_path = simplify_fasta(positive_path)\n",
        "    print(f'Simplifying the data for {negative_path} ...')\n",
        "    negative_simplified_path = simplify_fasta(negative_path)\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Read the simplified content\n",
        "    print(f'Reading simplified data from {positive_simplified_path} ...')\n",
        "    with open(positive_simplified_path, 'r') as file:\n",
        "        simplified_positive = file.read()\n",
        "    print(f'Reading simplified data from {negative_simplified_path} ...')\n",
        "    with open(negative_simplified_path, 'r') as file:\n",
        "        simplified_negative = file.read()\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Shuffle the data\n",
        "    print('Shuffling the positive sequences ...')\n",
        "    shuffled_positive = shuffle_fasta(simplified_positive)\n",
        "    print('Shuffling the negative sequences ...')\n",
        "    shuffled_negative = shuffle_fasta(simplified_negative)\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Split the data into entries\n",
        "    pos_entries = shuffled_positive.split('\\n\\n')\n",
        "    neg_entries = shuffled_negative.split('\\n\\n')\n",
        "\n",
        "    print(f'Number of positive sequences: {len(pos_entries)}')\n",
        "    print(f'Number of negative sequences: {len(neg_entries)}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Determine the index for splitting based on the training_portion\n",
        "    split_index_pos = int(len(pos_entries) * training_portion)\n",
        "    split_index_neg = int(len(neg_entries) * training_portion)\n",
        "\n",
        "    print(f'Training portion: {training_portion * 100}%')\n",
        "    print(f'Number of positive sequences in training set: {split_index_pos}')\n",
        "    print(f'Number of positive sequences in test set: {len(pos_entries) - split_index_pos}')\n",
        "    print(f'Number of negative sequences in training set: {split_index_neg}')\n",
        "    print(f'Number of negative sequences in test set: {len(neg_entries) - split_index_neg}')\n",
        "    print('-' * 40)\n",
        "\n",
        "    # Create training and testing datasets\n",
        "    pos_train_data = pos_entries[:split_index_pos]\n",
        "    pos_test_data = pos_entries[split_index_pos:]\n",
        "    neg_train_data = neg_entries[:split_index_neg]\n",
        "    neg_test_data = neg_entries[split_index_neg:]\n",
        "\n",
        "    # Shuffle the datasets\n",
        "    random.shuffle(pos_train_data)\n",
        "    random.shuffle(pos_test_data)\n",
        "    random.shuffle(neg_train_data)\n",
        "    random.shuffle(neg_test_data)\n",
        "\n",
        "    # Extract the base name without extension from the positive path and remove '_pos' if it exists\n",
        "    base_name = positive_path.rsplit('.', 1)[0]\n",
        "    base_name = base_name.replace('_pos', '')\n",
        "\n",
        "    # Create file names\n",
        "    pos_train_file_name = f'{base_name}_pos_train.fasta'\n",
        "    pos_test_file_name = f'{base_name}_pos_test.fasta'\n",
        "    neg_train_file_name = f'{base_name}_neg_train.fasta'\n",
        "    neg_test_file_name = f'{base_name}_neg_test.fasta'\n",
        "\n",
        "    # Save the datasets to files\n",
        "    print(f'Saving positive training data to {pos_train_file_name}')\n",
        "    with open(pos_train_file_name, 'w') as file:\n",
        "        file.write('\\n\\n'.join(pos_train_data))\n",
        "    print(f'Saving positive test data to {pos_test_file_name}')\n",
        "    with open(pos_test_file_name, 'w') as file:\n",
        "        file.write('\\n\\n'.join(pos_test_data))\n",
        "    print(f'Saving negative training data to {neg_train_file_name}')\n",
        "    with open(neg_train_file_name, 'w') as file:\n",
        "        file.write('\\n\\n'.join(neg_train_data))\n",
        "    print(f'Saving negative test data to {neg_test_file_name}')\n",
        "    with open(neg_test_file_name, 'w') as file:\n",
        "        file.write('\\n\\n'.join(neg_test_data))\n",
        "\n",
        "    print('Training and testing datasets have been saved separately for positive and negative sequences.')\n",
        "    print('-' * 40)"
      ],
      "metadata": {
        "id": "d729SL8DnEgr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for path in pos_list:\n",
        "    process_fasta_files(\n",
        "        positive_path=path,\n",
        "        negative_path=\"negative.fasta\",\n",
        "        training_portion=0.8\n",
        "    )\n",
        "    print('\\n\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhatK7wdspcg",
        "outputId": "7839f1a7-bc5e-4d8d-b717-9635ace9992b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------\n",
            "Simplifying the data for strict_pos_98.fasta ...\n",
            "File saved as strict_pos_98_simple.fasta\n",
            "Simplifying the data for negative.fasta ...\n",
            "File saved as negative_simple.fasta\n",
            "----------------------------------------\n",
            "Reading simplified data from strict_pos_98_simple.fasta ...\n",
            "Reading simplified data from negative_simple.fasta ...\n",
            "----------------------------------------\n",
            "Shuffling the positive sequences ...\n",
            "Shuffling the negative sequences ...\n",
            "----------------------------------------\n",
            "Number of positive sequences: 391\n",
            "Number of negative sequences: 570891\n",
            "----------------------------------------\n",
            "Training portion: 80.0%\n",
            "Number of positive sequences in training set: 312\n",
            "Number of positive sequences in test set: 79\n",
            "Number of negative sequences in training set: 456712\n",
            "Number of negative sequences in test set: 114179\n",
            "----------------------------------------\n",
            "Saving positive training data to strict_98_pos_train.fasta\n",
            "Saving positive test data to strict_98_pos_test.fasta\n",
            "Saving negative training data to strict_98_neg_train.fasta\n",
            "Saving negative test data to strict_98_neg_test.fasta\n",
            "Training and testing datasets have been saved separately for positive and negative sequences.\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Simplifying the data for strict_pos_80.fasta ...\n",
            "File saved as strict_pos_80_simple.fasta\n",
            "Simplifying the data for negative.fasta ...\n",
            "File saved as negative_simple.fasta\n",
            "----------------------------------------\n",
            "Reading simplified data from strict_pos_80_simple.fasta ...\n",
            "Reading simplified data from negative_simple.fasta ...\n",
            "----------------------------------------\n",
            "Shuffling the positive sequences ...\n",
            "Shuffling the negative sequences ...\n",
            "----------------------------------------\n",
            "Number of positive sequences: 391\n",
            "Number of negative sequences: 570891\n",
            "----------------------------------------\n",
            "Training portion: 80.0%\n",
            "Number of positive sequences in training set: 312\n",
            "Number of positive sequences in test set: 79\n",
            "Number of negative sequences in training set: 456712\n",
            "Number of negative sequences in test set: 114179\n",
            "----------------------------------------\n",
            "Saving positive training data to strict_80_pos_train.fasta\n",
            "Saving positive test data to strict_80_pos_test.fasta\n",
            "Saving negative training data to strict_80_neg_train.fasta\n",
            "Saving negative test data to strict_80_neg_test.fasta\n",
            "Training and testing datasets have been saved separately for positive and negative sequences.\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Simplifying the data for not_strict_pos_98.fasta ...\n",
            "File saved as not_strict_pos_98_simple.fasta\n",
            "Simplifying the data for negative.fasta ...\n",
            "File saved as negative_simple.fasta\n",
            "----------------------------------------\n",
            "Reading simplified data from not_strict_pos_98_simple.fasta ...\n",
            "Reading simplified data from negative_simple.fasta ...\n",
            "----------------------------------------\n",
            "Shuffling the positive sequences ...\n",
            "Shuffling the negative sequences ...\n",
            "----------------------------------------\n",
            "Number of positive sequences: 391\n",
            "Number of negative sequences: 570891\n",
            "----------------------------------------\n",
            "Training portion: 80.0%\n",
            "Number of positive sequences in training set: 312\n",
            "Number of positive sequences in test set: 79\n",
            "Number of negative sequences in training set: 456712\n",
            "Number of negative sequences in test set: 114179\n",
            "----------------------------------------\n",
            "Saving positive training data to not_strict_98_pos_train.fasta\n",
            "Saving positive test data to not_strict_98_pos_test.fasta\n",
            "Saving negative training data to not_strict_98_neg_train.fasta\n",
            "Saving negative test data to not_strict_98_neg_test.fasta\n",
            "Training and testing datasets have been saved separately for positive and negative sequences.\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "----------------------------------------\n",
            "Simplifying the data for not_strict_pos_80.fasta ...\n",
            "File saved as not_strict_pos_80_simple.fasta\n",
            "Simplifying the data for negative.fasta ...\n",
            "File saved as negative_simple.fasta\n",
            "----------------------------------------\n",
            "Reading simplified data from not_strict_pos_80_simple.fasta ...\n",
            "Reading simplified data from negative_simple.fasta ...\n",
            "----------------------------------------\n",
            "Shuffling the positive sequences ...\n",
            "Shuffling the negative sequences ...\n",
            "----------------------------------------\n",
            "Number of positive sequences: 391\n",
            "Number of negative sequences: 570891\n",
            "----------------------------------------\n",
            "Training portion: 80.0%\n",
            "Number of positive sequences in training set: 312\n",
            "Number of positive sequences in test set: 79\n",
            "Number of negative sequences in training set: 456712\n",
            "Number of negative sequences in test set: 114179\n",
            "----------------------------------------\n",
            "Saving positive training data to not_strict_80_pos_train.fasta\n",
            "Saving positive test data to not_strict_80_pos_test.fasta\n",
            "Saving negative training data to not_strict_80_neg_train.fasta\n",
            "Saving negative test data to not_strict_80_neg_test.fasta\n",
            "Training and testing datasets have been saved separately for positive and negative sequences.\n",
            "----------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxsB8oegyrzM",
        "outputId": "5460f36b-120b-4534-8101-af2c617bdf3c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['strict_pos_98.fasta',\n",
              " 'strict_pos_80.fasta',\n",
              " 'not_strict_pos_98.fasta',\n",
              " 'not_strict_pos_80.fasta']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names = []\n",
        "extensions = [\n",
        "    '_pos_train.fasta',\n",
        "    '_pos_test.fasta',\n",
        "    '_neg_train.fasta',\n",
        "    '_neg_test.fasta'\n",
        "]\n",
        "for raw_name in pos_list:\n",
        "    for i, extension in enumerate(extensions):\n",
        "        dataset_names.append(raw_name.split('.')[0].replace('_pos', '') + extension)"
      ],
      "metadata": {
        "id": "e2QIuX-TxR-u"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nxY5IeEyiPD",
        "outputId": "c05f67bf-66e3-46e4-c754-79e0234aba3d"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['strict_98_pos_train.fasta',\n",
              " 'strict_98_pos_test.fasta',\n",
              " 'strict_98_neg_train.fasta',\n",
              " 'strict_98_neg_test.fasta',\n",
              " 'strict_80_pos_train.fasta',\n",
              " 'strict_80_pos_test.fasta',\n",
              " 'strict_80_neg_train.fasta',\n",
              " 'strict_80_neg_test.fasta',\n",
              " 'not_strict_98_pos_train.fasta',\n",
              " 'not_strict_98_pos_test.fasta',\n",
              " 'not_strict_98_neg_train.fasta',\n",
              " 'not_strict_98_neg_test.fasta',\n",
              " 'not_strict_80_pos_train.fasta',\n",
              " 'not_strict_80_pos_test.fasta',\n",
              " 'not_strict_80_neg_train.fasta',\n",
              " 'not_strict_80_neg_test.fasta']"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## HMM"
      ],
      "metadata": {
        "id": "SVnGGH_zToMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67oRXIp7vaQU",
        "outputId": "8d4ff6f6-8797-4cc5-bbb0-51f668b28a12"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['strict_pos_98.fasta',\n",
              " 'strict_pos_80.fasta',\n",
              " 'not_strict_pos_98.fasta',\n",
              " 'not_strict_pos_80.fasta']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_98_pos_train.out strict_msa.hmm strict_98_pos_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_80_pos_train.out strict_msa.hmm strict_80_pos_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_98_pos_test.out strict_msa.hmm strict_98_pos_test.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_80_pos_test.out strict_msa.hmm strict_80_pos_test.fasta > /dev/null 2>&1\n",
        "\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_98_pos_train.out not_strict_msa.hmm not_strict_98_pos_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_80_pos_train.out not_strict_msa.hmm not_strict_80_pos_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_98_pos_test.out not_strict_msa.hmm not_strict_98_pos_test.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_80_pos_test.out not_strict_msa.hmm not_strict_80_pos_test.fasta > /dev/null 2>&1\n",
        "\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_98_neg_train.out strict_msa.hmm strict_98_neg_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_80_neg_train.out strict_msa.hmm strict_80_neg_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_98_neg_test.out strict_msa.hmm strict_98_neg_test.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout strict_80_neg_test.out strict_msa.hmm strict_80_neg_test.fasta > /dev/null 2>&1\n",
        "\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_98_neg_train.out not_strict_msa.hmm not_strict_98_neg_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_80_neg_train.out not_strict_msa.hmm not_strict_80_neg_train.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_98_neg_test.out not_strict_msa.hmm not_strict_98_neg_test.fasta > /dev/null 2>&1\n",
        "!hmmsearch -Z 1 --domZ 1 --max --noali --tblout not_strict_80_neg_test.out not_strict_msa.hmm not_strict_80_neg_test.fasta > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "BKgDJ3TMI7OA"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp strict_98_neg_train.out /content/drive/MyDrive/lab_of_bioinformatics/strict_98_neg_train.out\n",
        "!cp strict_80_neg_train.out /content/drive/MyDrive/lab_of_bioinformatics/strict_80_neg_train.out\n",
        "!cp strict_98_neg_test.out /content/drive/MyDrive/lab_of_bioinformatics/strict_98_neg_test.out\n",
        "!cp strict_80_neg_test.out /content/drive/MyDrive/lab_of_bioinformatics/strict_80_neg_test.out\n",
        "\n",
        "!cp not_strict_98_neg_train.out /content/drive/MyDrive/lab_of_bioinformatics/not_strict_98_neg_train.out\n",
        "!cp not_strict_80_neg_train.out /content/drive/MyDrive/lab_of_bioinformatics/not_strict_80_neg_train.out\n",
        "!cp not_strict_98_neg_test.out /content/drive/MyDrive/lab_of_bioinformatics/not_strict_98_neg_test.out\n",
        "!cp not_strict_80_neg_test.out /content/drive/MyDrive/lab_of_bioinformatics/not_strict_80_neg_test.out"
      ],
      "metadata": {
        "id": "Q1DuqbUq6QeX"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v \"^#\" strict_98_pos_train.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > strict_98_pos_train.data\n",
        "!grep -v \"^#\" strict_80_pos_train.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > strict_80_pos_train.data\n",
        "!grep -v \"^#\" strict_98_pos_test.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > strict_98_pos_test.data\n",
        "!grep -v \"^#\" strict_80_pos_test.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > strict_80_pos_test.data\n",
        "\n",
        "!grep -v \"^#\" not_strict_98_pos_train.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > not_strict_98_pos_train.data\n",
        "!grep -v \"^#\" not_strict_80_pos_train.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > not_strict_80_pos_train.data\n",
        "!grep -v \"^#\" not_strict_98_pos_test.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > not_strict_98_pos_test.data\n",
        "!grep -v \"^#\" not_strict_80_pos_test.out | awk '{print $1\"\\t\"$8\"\\t1\" }' > not_strict_80_pos_test.data"
      ],
      "metadata": {
        "id": "HMkKmqFd8b2t"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v \"^#\" strict_98_neg_train.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_strict_98_neg_train.data\n",
        "!grep -v \"^#\" strict_80_neg_train.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_strict_80_neg_train.data\n",
        "!grep -v \"^#\" strict_98_neg_test.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_strict_98_neg_test.data\n",
        "!grep -v \"^#\" strict_80_neg_test.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_strict_80_neg_test.data\n",
        "\n",
        "!grep -v \"^#\" not_strict_98_neg_train.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_not_strict_98_neg_train.data\n",
        "!grep -v \"^#\" not_strict_80_neg_train.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_not_strict_80_neg_train.data\n",
        "!grep -v \"^#\" not_strict_98_neg_test.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_not_strict_98_neg_test.data\n",
        "!grep -v \"^#\" not_strict_80_neg_test.out | awk '{print $1\"\\t\"$8\"\\t0\" }' > tmp_not_strict_80_neg_test.data"
      ],
      "metadata": {
        "id": "1nhRGYC961_k"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -v \"^#\" not_strict_80_neg_test.out | awk '{print $1\"\\t\"$8\"\\t0\" }' | sort -grk 2 | head -n 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yxwxj3Sg9WDW",
        "outputId": "b27fc3b8-4137-4644-83df-d604d0c674a3"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X6R8R1\t1\t0\n",
            "X2L4E2\t1\t0\n",
            "W6QIT2\t1\t0\n",
            "W5PLZ6\t1\t0\n",
            "W5DYE3\t1\t0\n",
            "W4VSI5\t1\t0\n",
            "W4VRU5\t1\t0\n",
            "W3XC32\t1\t0\n",
            "U5PZT6\t1\t0\n",
            "U5L3M7\t1\t0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat not_strict_98_pos_test.data | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9VhDTe46195",
        "outputId": "e3821594-0964-46c0-e4eb-4a8ed4186e40"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "O76840\t1.1e-23\t1\n",
            "P10646\t4e-29\t1\n",
            "P19761\t1.9e-28\t1\n",
            "P83606\t9.1e-26\t1\n",
            "Q7YRQ8\t1.3e-30\t1\n",
            "B6ZIW0\t4.2e-30\t1\n",
            "P04365\t4.3e-26\t1\n",
            "P02760\t7.1e-26\t1\n",
            "Q08E66\t2.1e-27\t1\n",
            "P00974\t1.5e-33\t1\n",
            "A0A1D0BND9\t2.1e-18\t1\n",
            "Q90WA1\t2.3e-32\t1\n",
            "P0DMW7\t7.1e-32\t1\n",
            "Q6T6T5\t2.3e-31\t1\n",
            "C0HK74\t4.3e-31\t1\n",
            "Q7LZE3\t8.8e-31\t1\n",
            "F8J2F3\t1.2e-30\t1\n",
            "P0DN06\t1.5e-30\t1\n",
            "B5L5R0\t2.1e-30\t1\n",
            "P31713\t2.9e-30\t1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-NkdfCC617c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "36p6iOHf615V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7qACi4k3612F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vhfnXOgXWMN1"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Dqk8Qq_1qmkB"
      },
      "outputs": [],
      "source": [
        "#The line of the code below initiates a BLASTP search, a tool for comparing protein sequences. It takes protein sequences from \"output_seq.fasta\" as the query and compares them against a database specified in \"bpti_reviewd.fasta\". The results are saved in \"bpti_reviewd.blast\" using format 7, which is suitable for further analysis.\n",
        "!blastp -query strict_pos_98.fasta -db strict_seqs.fasta -out strict_pos_98.blast -outfmt 7\n",
        "!blastp -query strict_pos_80.fasta -db strict_seqs.fasta -out strict_pos_80.blast -outfmt 7\n",
        "!blastp -query not_strict_pos_98.fasta -db strict_seqs.fasta -out not_strict_pos_98.blast -outfmt 7\n",
        "!blastp -query not_strict_pos_80.fasta -db strict_seqs.fasta -out not_strict_pos_80.blast -outfmt 7"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat  strict_bpti.blast | head -n 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60nOBGbYopQM",
        "outputId": "bc7fc8be-3714-45b7-bc5c-1135345a080a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat: strict_bpti.blast: No such file or directory\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}